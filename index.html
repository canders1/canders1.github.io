---
layout: default
---

<!-- Page Content -->
<div class="container">

    <!-- Portfolio Item Row -->
    <div class="row">
        <div class="row">
            <div class="col-md-7">
                <h3>Welcome</h3>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8">
                <p>I am a computational linguist and an assistant professor of Computer Science at Wellesley College.</p>
                <h3>Research</h3>
                <p>My research focuses on understanding how context-sensitive meaning is encoded in natural language. I build computational models to understand how conversation participants use knowledge about each other's mental states. How do speakers think about their audience when deciding what to say? How do listeners use their knowledge of the speaker when figuring out the meaning of their utterances?</p>

                <p>More recently, I have been focusing on evaluating large language models for natural language and code generation. What are the abilities and limitations of LLMs? Can LLMs help non-expert programmers? I study these questions from a variety of angles, including model development, benchmarking and evaluation, and human-computer interaction studies.</p>

                 <!--<p>My dissertation, <b><a href="https://ling.auf.net/lingbuzz/005454?_s=6KVdJSTSZvacd4I_&_k=cqAtx-8HmlYak3QH">Shifting the Perspectival Landscape</a></b>, is now on <a href="https://ling.auf.net/lingbuzz/005454?_s=6KVdJSTSZvacd4I_&_k=cqAtx-8HmlYak3QH">LingBuzz</a>.</p>-->
            </div>
            <div class="col-md-1">
            </div>
            <div class="col-md-3">
                <a href="#">
                    <img class="img-responsive thumbnail" src="img/ta_small.jpg">
                </a>
                <div style="display: grid; justify-content:center">carolyn.anderson AT wellesley.edu</div>
                <div style="display: grid; justify-content:center"><h6></h6></div>
                <!--<div style="display: grid; justify-content:center">
                    <h3 class="interests">Interests</h3>
                    <h5 class="interests">o Computational Modeling</h5>
                    <h5 class="interests">o Semantics and Pragmatics</h5>
                    <h5 class="interests">o Machine Learning</h5>
                    <h5 class="interests">o Psycholinguistics</h5>
                </div>-->
            </div>
        </div>
        <div class="row">
            <div class="col-md-10">
                <h3>News</h3>
                <p>o February 2023: I visited Swarthmore to give a colloquium in the Linguistics department</p>
                <p>o February 2023: <b>Parenthesized Modifiers in English and Korean: What They (May) Mean</b> accepted to ELM 3</p>
                <p>o January 2023: <b>How Beginning Programmers and Code LLMs (Mis)read Each Other</b> accepted to CHI 2024</p>
                <p>o January 2023: <a href="https://arxiv.org/abs/2312.12450"><b>Can It Edit?</b></a> accepted to LLM4Code workshop at ICSE 2024</p>
                <p>o January 2023: <a href="https://arxiv.org/abs/2306.04556"><b>StudentEval</b></a> accepted to LLM4Code workshop at ICSE 2024</p>
                <p>o November 2023: <a href="https://arxiv.org/abs/2305.06161"><b>StarCoder</b></a> accepted to <i>Transactions on Machine Learning Research</i></p>
                <p>o November 2023: Three papers by EASEL lab alums presented at TADA 2023</p>
                <p>o September 2023: <b>Cross-linguistic differences in processing parentheticals between English and Korean</b> presented at Comparative Punctuation</p>
                <p>o September 2023: <b>Protagonist-Mediated Perspective</b> presented at Sinn und Bedeutung 28.</p>
                <p>o August 2023: Received an NSF Award for research on Code LLMs for programming in the sciences, with Molly Q Feldman, Arjun Guha, and Erin G. Teich</p>
                <p>o August 2023: Our <a href="https://www.khoury.northeastern.edu/~arjunguha/main/papers/2023-multipl-t.html"><b>MultiPL-T paper</b></a> shows how to achieve SOTA text-to-code in 3 low-resource programming languages</p>
                <!--<p>o July 2023: I presented a poster on <b>What (Some) Parentheses Mean</b> at UMass Linguistic's 50th anniversary celebration</p>
                <p>o July 2023: I gave a <a href="https://indico.bnl.gov/event/20122/">talk</a> as part of Brookhaven National Lab's AI/ML Seminar Series</p>
                <p>o June 2023: <b>Do All Minority Languages Look the Same to GPT-3?</b> presented at SCiL 2023</p>
                <p>o June 2023: <b>Solving and Generating NPR Sunday Puzzles with Large Language Models</b> presented at ICCC 2023.</p>
                <p>o June 2023: <b>StudentEval: a Benchmark of Student-Written Prompts for Large Language Models of Code</b> draft available on Arxiv.</p>
                <p>o May 2023: BigCode project released <b><a href="https://huggingface.co/bigcode/starcoder">StarCoder</a>: May the Source Be With You!</b></p>
                <p>o April 2023: <a href="https://arxiv.org/abs/2208.08227">MultiPL-E</a> accepted to <i>IEEE Transactions on Software Engineering</i></p>
                <p>o March 2023: <b>SantaCoder: Donâ€™t Reach For the Stars!</b> wins Best Paper Award at DL4C 2023.</p>
                <p>o December 2022: <b>Grammatical Perspective-Taking in Comprehension and Production</b> accepted to <i>Open Mind</i>.</p>
                <p>o May 2023: EASEL lab member Michelle Zhao successfully defends her honors thesis. Congrats!</p>
                <p>o December 2022: I was invited to participate in the AAAI/ACM SIGAI New and Future AI Educator Program.</p>
                <p>o December 2022: EASEL lab member Skylar Kolisko successfully defends her honors thesis: <b>Names in Large Language Models</b>.</p>
                <p>o November 2022: <b>Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course</b> accepted to Educational Advances in Artificial Intelligence (EAAI) 2023.</p>
                -->
            </div>
        </div>
    </div>
</div>